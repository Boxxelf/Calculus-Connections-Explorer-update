Machine Learning,,
Calculus topic,CS topic,Rationale
Calculus I,,
Motivating the need for calculus & limits,Working with data,"Isaac Newton's development of core calculus ideas was in part inspired by his wish to understand laws of physics, as in Philosophiæ Naturalis Principia Mathematica. These laws were designed to be consistent with data. Gauss's work on predicting celestial mechanics from data is grounded in calculus. Some of the earliest roots of calculus are grounded in data analysis. "
Motivating the need for calculus & limits,Gradient descent,Computing derivatives is essential for gradient descent.
Motivating the need for calculus & limits,Regression,Regression often tries to minimize the function that represents the sum of squared errors. Finding the minima of a function involves computing derivatives.
Motivating the need for calculus & limits,Clustering,"In k-means, for instance, the key step is finding the mean of a collection of points, which corresponds to finding the point which minimizes the sum of squared differences."
Motivating the need for calculus & limits,Neural networks,"Backpropagation, which is used to train neural networks, is built on calculus."
Motivating the need for calculus & limits,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Limits at infinity and infinite limits,Overfitting/underfitting,"Analyzing the limit of a sequence might come up when graphing a loss function in terms of a hyperparameter or epoch. For example, we might be interested to see if the loss converges or we might be interested in the points where the loss has a local minimum (a trough)."
Limits at infinity and infinite limits,Gradient descent,Gradient descent is esentially a sequence in the parameter space. We are interested in the convergence of that sequence towards a local minimum of the loss function.
Limits at infinity and infinite limits,Regularization,"In ridge/lasso regression, in the plot of standardized coefficients in terms of lambda, the coefficient curves will approach zero when lambda will approach infinity."
Limits at infinity and infinite limits,Learning theory,"Learning theory topics are typically focused on asymptotic behavior (for example, as the number of samples grows, will we converge on the right model?) This requires understanding the idea of a limit to infinity."
Motivating the need for the derivative and introducing the derivative concept,Gradient descent,"Gradient descent is based on gradient, which is a vector that consists of (partial) derivatives."
Motivating the need for the derivative and introducing the derivative concept,Model evaluation techniques,Understand the sensitivity of model predictions to input changes
Defining the derivative as a function,Gradient descent,"Understanding gradient descent requires understanding the idea of a gradient as a function that we can use to move along an error surface. Since the gradient generalizes a derivative, understanding the derivative as a function is also helpful here."
Defining the derivative as a function,Neural Networks,Autodifferentiation is key to backpropagation. It works because neural networks are compositions of basic functions with known derivatives.
Defining the derivative as a function,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Basic differentiation rules,Gradient descent,Performing gradient descent requires calculating (partial) derivatives. This requires one to apply basic differentiation rules.
Basic differentiation rules,Regression problems,"To find the values that minimize the loss, we  need to be able to differentiate the loss function."
Basic differentiation rules,Classification problems,"For some particular models/algorithms used for classification, we typically use differentation to find the extreme values. For other models (for example, discrete models such decision trees), we do not use differentiation to find the extreme values."
Basic differentiation rules,Neural Networks,Autodifferentiation is key to backpropagation. It works because neural networks are compositions of basic functions with known derivatives.
Basic differentiation rules,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Basic differentiation rules,Probabilistic models,"To fit something like Naïve Bayes, you need basic differentiation rules. (You also need Lagrange multipliers, but that is a Calc III topic.)"
Basic differentiation rules,Topic models,"Topic models are often fit with expectation maximization. Deriving the maximization step requires optimizing a function, often by taking the derivative."
Product and quotient rules,Gradient descent,"Performing gradient descent requires calculating (partial) derivatives. This requires one to apply basic differentiation rules, including product and quotient rules."
Trigonometric derivatives,Neural networks,When using sinusoidal and tanh activation functions
Trigonometric derivatives,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Derivatives of logarithmic and exponential functions,Classification problems,"In logistic regression, we need to compute the derivative of the loss function, which is a function that involves exponentials and logs."
Chain rule,Neural networks,"Backpropagation updates the weights in a neural network using a process that is makes use of the chain rule. Autodifferentiation key to backpropagation, works because neural networks are compositions of basic functions with known derivatives."
Chain rule,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Extreme values,Overfitting/underfitting,"Analyzing the limit of a sequence might come up when graphing a loss function in terms of a hyperparameter or epoch. For example, we might be interested to see if the loss converges or we might be interested in the points where the loss has a local minimum (a trough)."
Extreme values,Gradient descent,Gradient descent is esentially a sequence in the parameter space. We are interested in the convergence of that sequence towards a local minimum of the loss function.
Extreme values,Regression problems,"To fit a regression model, we need to optimize a loss function: find the parameter values that minimize the loss (or sometimes maximize the likelihood)"
Extreme values,Classification problems,"When using a classification model that is not a discrete model, we need to optimize a loss function or likelihood function, which requires finding the values of the parameters that achieve a min or max. No differentiation is needed when the model is discrete."
Extreme values,Probabilistic models,Finding the parameters that maximize the likelihood or the posterior requires finding extreme values of a function.
Extreme values,Topic models,Topic models are often fit with expectation maximization. Deriving the maximization step requires finding the extreme value that maximizes  a function.
The shape of graphs (concavity),Gradient descent,Gradient descent is esentially a sequence in the parameter space. We are interested in the convergence of that sequence towards a local minimum of the loss function. 
Optimization,Gradient descent,Gradient descent is all about optimizing a function by using gradients. This is closely related to optimizing a function using derivatives (since the gradient just generalizes the derivative)
Optimization,Regularization,There are many settings with non-unique minimizers unless you use regularization.
Optimization,Regression problems,"To fit the weights for a regression problem, we optimize a loss function by finding a closed form for the min value. This is analogous to other calculus examples of optimization."
Optimization,Classification problems,"To fit a classification problem, we optimize either the likelihood or loss function by setting the parameter values. This is analogous to other calculus examples of optimization."
Optimization,Neural Networks,Identifying the weights in a neural network is framed as an optimization problem.
Optimization,Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Optimization,Probabilistic models,"Fitting the parameters of a probabilistic model typically happens via optimization (e.g., finding the parameters to achieve the MAP or MLE )."
Optimization,Graphical models,Graphical models framed as optimization problems.
Optimization,Topic models,Topic models are often fit with expectation maximization. Deriving the maximization step requires optimizing a function.
Newton's method,Gradient descent,"Explain the difference between first-order and second-order optimization, role of second derivative"
Motivating the need for integrals and approximating the area under curves,Model evaluation techniques,Area under the curve (AUC) to evaluate ML models; also p-values
Motivating the need for integrals and approximating the area under curves,Regression problems,"In linear regression, the value of area under a curve comes up in the computation of the confidence intervals. "
Motivating the need for integrals and approximating the area under curves,Classification problems,Area under the curve metric for classifiers; also the error function for Gaussian data
Definite integrals,Model evaluation techniques,"Models are commonly evaluated using the AUC curve, and integrals are helpful for understanding the idea of calculating the area under a curve."
Integrals of exponential and logarithmic functions,Model evaluation techniques,Error function for Gaussian data
Hyperbolic functions (derivatives and integrals),Neural networks,When using sinusoidal and tanh activation functions
Hyperbolic functions (derivatives and integrals),Advanced Deep Learning,Advanced Deep Learning requires at least as much knowledge as Neural Networks.
Calculus II,,
Applications to probability,Model evaluation techniques,"Type I and type 2 errors in classification, p-values, error functions"
Applications to probability,Bias-variance tradeoff,"The bias variance tradeoff requires understanding probabilistic concepts like a probability density function, mean, and variance."
Applications to probability,Classification problems,"Understanding probabilistic classifiers, such as Naïve Bayes, requires some knowledge of probability."
Applications to probability,Regression problems,"In linear regression, the value of area under a curve comes up in the computation of the confidence intervals. Also, maximum likelihood estimation often comes up in the context of  regression, and understanding the likelihood means understanding some probability, for example, relying on Gaussian likelihoods."
Limits at infinity and infinite limits,Gradient descent,Gradient descent is esentially a sequence in the parameter space. We are interested in the convergence of that sequence towards a local minimum of the loss function.
Sequences,Learning theory,Interested in limit as amount of data -> infinity. 
Series,Regression problems,Autoregressive model of a time series
Convergence and divergence,Gradient descent,Gradient descent will converge or diverge depending on step size
Taylor series,Gradient descent,First-order vs second-order optimization methods
Polar coordinates,Working with data,"Understanding different ways of representing data (e.g. polar vs cartesian coordinates) can be important for choosing features and/or transforming the data before performing machine learning on it.  On a related note, some machine learning methods use nonlinear transformations to change the data into a higher dimensional space where it becomes linearly separable (e.g. kernel methods in SVMs)."
Algorithms,,
Calculus topic,CS topic,"Rationale (Note: Unless mentioned otherwise, all chapters and sections refer to Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms, 4th edition.)"
Calculus I,,
Introducing the limit concept,Characterizing Running Times (§3),The basic concept of a limit is absolutely critical to understanding how we are trying to characterize end-behavior / growth rates of a function as n -> infinity.
Determining the limits of functions (limit laws),Characterizing Running Times (§3),Asymptotic comparison of polynomial and exponential functions is done through computing a limit. (page 65). e^x is described as a limit. (page 67). Asymptotic comparison of polynomial and exponential functions.  e^x described as the limit of (1 + x/n)^n when n --> infinity. 
Determining the limits of functions (limit laws),Probabilistic Analysis and Randomized Algorithms (§5),"In 5.12, the understanding of an exponential as a limit is used to give an upper bound. (page 147). Uses the limit e^x = limit of (1 + x/n)^n when n --> infinity to get an upper bound."
Determining the limits of functions (limit laws),Approximation Algorithms (§35),"In the line after 35.27, the exponential is described as a limit. (page 1129) e^x described as the limit of (1 + x/n)^n when n --> infinity"
Limits at infinity and infinite limits,Characterizing Running Times (§3),Limits are used to define small-oh and small-omega. (pages 60-61). Small-oh and small-omega are defined using limits at infinity
Limits at infinity and infinite limits,"Divide-and-Conquer (§4) [§4 (intro), §4.1, §4.2, §4.4, §4.5, CPP (closest pair of points, §33.4 from 3rd edition)]",The cases in the master theorem depend on some functions growing polynomial faster than other functions. (page 103) Asymptotic comparison of polynomial and exponential functions 
Limits at infinity and infinite limits,Probabilistic Analysis and Randomized Algorithms (§5),"In 5.12, the understanding of an exponential as a limit is used to give an upper bound. (page 147). Uses the limit e^x = limit of (1 + x/n)^n when n --> infinity to get an upper bound."
Limits at infinity and infinite limits,Approximation Algorithms (§35),"In the line after 35.27, the exponential is described as a limit. (page 1129) e^x described as the limit of (1 + x/n)^n when n --> infinity"
Motivating the need for the derivative and introducing the derivative concept,Matrix Operations  (§28),The fact that a local minimum is a critical point is used. (page 842). Use critical points to locate local extrema (learning objective 4.3.4)
Basic differentiation rules,Matrix Operations  (§28),The derivative of the square of the norm of the error vector is computed. (page 842) Uses differentiation power rule.
Basic differentiation rules,Approximation Algorithms (§35),"In 35.28, a derivative is computed to show that a function is increasing. (page 1129) Use the positivity of the derivative to conclude that the function is increasing"
The shape of graphs (concavity),Approximation Algorithms (§35),"In 35.28, a derivative is computed to show that a function is increasing. (page 1129) Use the positivity of the derivative to conclude that the function is increasing"
L'Hopitals rule (using derivatives to evaluate limits of indeterminate form),Characterizing Running Times (§3),"The characterization of a running time (e.g., in the definitions for little o and little omega) often involves the ratio of two functions that go to infinity, which is covered in the L'Hopitals rule section."
Optimization,Matrix Operations  (§28),The linear regression is an optimization problem. (pages 841-844)
Motivating the need for integrals and approximating the area under curves,Summations (Appendix A),"On pages 1150-1151, the authors discuss how it can sometimes be useful to bound discrete summations by using integrals.  The figures on page 1151 should look VERY familiar to anyone who has taught how integrals and Reimann sums relate to areas under a curve!"
Motivating the need for integrals and approximating the area under curves,Probabilistic Analysis and Randomized Algorithms (§5),Use of integrals to bound a discrete sum is based on the understanding of how integrals relate to Riemann sums.
Calculus II,,
Sequences,Summations (Appendix A),Convergence of series is defined in terms of the convergence of partial sum sequence. (page 1140) Geometric and harmonic series are presented. (page 1142). Definition of a convergent series. Geometric series
Series,Summations (Appendix A),Convergence of series is defined in terms of the convergence of partial sum sequence. (page 1140) Geometric and harmonic series are presented. (page 1142). Definition of a convergent series. Geometric series
Series,"Divide-and-Conquer (§4) [§4 (intro), §4.1, §4.2, §4.4, §4.5, CPP (closest pair of points, §33.4 from 3rd edition)]",Geometric series is used to compute an upper bound. (page 97)
Series,"Dynamic Programming (§14) [§14.1, §14.2, §14.3, §14.4, §14.5]",Geometric series is used to compute a lower bound. (page 389)
Series,Quicksort (§7),A harmonic series is used to compute a bound. (page 184)
Series,Medians and Order Statistics (§9),A geometric series is used to give an upper bound. (page 235)
Series,Hash Tables (§11),A geometric series is used to give an upper bound. (pages 298-299)
Convergence and divergence,Summations (Appendix A),Convergence of series is defined in terms of the convergence of partial sum sequence. (page 1140) Geometric and harmonic series are presented. (page 1142). Definition of a convergent series. Geometric series
Comparison tests,Summations (Appendix A),"Some series are obtained by integrating or differentiating both sides of other known series, such as the geometric series. (page 1142) Some series can be approximated by integrals (pages 1150-1151). Differentiate and integrate power series term-by-term. Use the integral test to determine the convergence of a series."
Comparison tests,Probabilistic Analysis and Randomized Algorithms (§5),The series is approximated by integrals in order to get lower and upper bounds. (page 152) Use ideas related to the integral test to get lower and upper bound 
Comparison tests,Heapsort (§6),A series is used to give an upper bound. The series is computed by differentiating the geometric series. (page 169) Uses the value of a series obtained by differentiating the power series to give an upper bound
Comparison tests,Hash Tables (§11),The series is approximated by integrals in order to get an upper bound. (page 300) Use ideas related to the integral test to get lower and upper bound 
Power series and functions,Summations (Appendix A),"Some series are obtained by integrating or differentiating both sides of other known series, such as the geometric series. (page 1142) Some series can be approximated by integrals (pages 1150-1151). Differentiate and integrate power series term-by-term. Use the integral test to determine the convergence of a series."
Artificial Intelligence,,
Calculus topic,CS topic,"Rationale (Note: Chapters, sections, and pages refer to Russell, Norvig, Artificial Intelligence, 4th edition.)"
Calculus I,,
Introducing the limit concept,Probabilistic Reasoning (§13),A limit converges to the expected value. (page 455) Write expected value as a limit
Introducing the limit concept,Multiagent Decision Making (§17),The utility is defined as a limit. (page 605)
Introducing the limit concept,Probabilistic Programming (§18),"Questions of whether the MCMC is ""well mixed"" are implicitly about the rate at which a function approaches a limit/asymptote.  (page 663) ""It could also be that MCMC inference has not mixed properly: if we ran 300 chains for 25 thousand or 25 million iterations, we might find a quite different
distribution of results, perhaps indicating that the first letter is probably u rather than q."""
Determining limits of functions (graphically and numerically),Reinforcement Learning (§23),The idea of the value function converging in the limit is discussed.
Limits at infinitiy and infinite limits,Multiagent Decision Making (§17),The utility is defined as a limit going to infinity. (page 605)
Limits at infinitiy and infinite limits,Probabilistic Reasoning (§13),"The concept of a ""stationary distribution"" implicitly requires a basic understanding of the idea of a limit as time goes to infinity."
Limits at infinitiy and infinite limits,Probabilistic Reasoning over Time (§14),"The concept of a ""stationary distribution"" implicitly requires a basic understanding of the idea of a limit as time goes to infinity."
Limits at infinitiy and infinite limits,Learning from Examples (§19),"The ML version of this argues that understanding training over time requires this topic, so the same argument applies here. Further, learning theory and gradient descent are considering a sequence of points and what happens in the limit to that sequence. The chapter also discusses no-regret learning and what happens to an algorithm asymptotically."
Limits at infinitiy and infinite limits,Computer Vision (§27),A limit as lambda goes to infinity is considered. (page 991) A limit to infinity is computed.
Motivating the need for the derivative and introducing the derivative concept,Search in Complex Environments (§4),One local search technique for continous spaces is steepest-hill hill climbing (aka gradient ascent) uses gradient and partial derivatives. (pages 137-139). Another technique is the Newton-Raphson method which uses the derivative. (page 139) Understand that the gradient consists of several (partial) derivatives. Use differential rules. Use Newton's method
Motivating the need for the derivative and introducing the derivative concept,Deep Learning (§22),Need to understand the overall concept of derivative (as rate of change) to understand what backprop is doing (propogating error depending on the rate of change attributed to a given neural unit).
Motivating the need for the derivative and introducing the derivative concept,Reinforcement Learning (§23),"The partial derivatives of the error function are used to adjust the parameters theta_i. (pages 855, 856) The differentiation power rule is used to compute a derivative"
Motivating the need for the derivative and introducing the derivative concept,Robotics (§26),Calculus of variations is used to express J(s) as an integral in terms of theta and its derivative. (page 956) Understand derivative notation
Defining the derivative as a function,Computer Vision (§27),Edge detection with a Gaussian convolution involves calculating gradients (page 998) and understanding how they capture rates / direction of change.
Basic differentiation rules,Search in Complex Environments (§4),One local search technique for continous spaces is steepest-hill hill climbing (aka gradient ascent) uses gradient and partial derivatives. (pages 137-139). Another technique is the Newton-Raphson method which uses the derivative. (page 139) Understand that the gradient consists of several (partial) derivatives. Use differential rules. Use Newton's method
Basic differentiation rules,Making Simple Decisions (§15),The power rule is used. (page 527) Use differential rules
Basic differentiation rules,Learning from Examples (§19),"The loss for linear regression is computed. (pages 695, 696) The derivative of the logistic function is computed. (page 704). The loss for linear regression is computed using differential rules. The derivative of the logistic function is computed using differential rules"
Basic differentiation rules,Learning Probabilistic Models (§21),Finding the maximum likelihood estimate requires using basic differentation rules to compute derivatives.
Basic differentiation rules,Deep Learning (§22),Computing derivatives of loss requires using basic differentiation rules as well as the chain rule.
Basic differentiation rules,Reinforcement Learning (§23),"The partial derivatives of the error function are used to adjust the parameters theta_i. (pages 855, 856) The differentiation power rule is used to compute a derivative"
Derivatives of logarithmic and exponential functions,Learning from Examples (§19),"The loss for linear regression is computed. (pages 695, 696) The derivative of the logistic function is computed. (page 704). The loss for linear regression is computed using differential rules. The derivative of the logistic function is computed using differential rules"
Derivatives of logarithmic and exponential functions,Learning Probabilistic Models (§21),The derivative of the log likelihood is computed. (page 776) Partial derivative of the loss function is computed. (page 777) Partial derivative of the loss function is computed. (page 780) The derivative of several log functions are computed using differential rules.
The chain rule,Learning from Examples (§19),"This chapter introduces us to gradient descent, including calculating partial derivatives of a loss function using the chain rule."
The chain rule,Learning Probabilistic Models (§21),"The example of finding the maximum likelihood estimate requires using the chain rule (and more generally, this commonly occurs in MLEs)."
The chain rule,Deep Learning (§22),The chain rule is used to compute the partial derivatives of the loss function. (page 806) The chain rule is used to compute partial derivatives. (page 818) The chain rule is used to compute partial derivatives. (page 825) 
Applications of derivatives: rates of change & exponential models,Computer Vision (§27),"Edge detection with a Gaussian convolution involves calculating gradients (page 998) and understanding how they capture rates / direction of change.  Similarly, optical flow (page 1000) is expressed as a difference in distances at different times, giving the idea of rates as a function of spots in the image. "
The shape of graphs (concavity),Deep Learning (§22),"The discussions of activation functions on page 804 includes discussion of derivatives and function shape: ""Notice that all of them are monotonically nondecreasing,
which means that their derivatives g' are nonnegative"".  (see also the discussion of the ""vanishing gradient"" problem at the top of page 807.  The shape of various activation functions has proven to be quite important in the study of neural networks and deep learning!"
Optimization,Learning from Examples (§19),Optimizing parameters over a loss function involves applied optimization.
Optimization,Learning Probabilistic Models (§21),Optimizing parameters to identify an MLE or MAP estimate is an optimization
Optimization,Deep Learning (§22),Optimizing parameters over a loss function involves applied optimization.
Optimization,Robotics (§26),Optial control focuses on using gradients for optimization.
Newton's method,Search in Complex Environments (§4),One local search technique for continous spaces is steepest-hill hill climbing (aka gradient ascent) uses gradient and partial derivatives. (pages 137-139). Another technique is the Newton-Raphson method which uses the derivative. (page 139) Understand that the gradient consists of several (partial) derivatives. Use differential rules. Use Newton's method
Definite integrals,Probabilistic Reasoning (§13),"As discussed in Application to probability, a soft threshold is defined using the integral of the standard normal. Having some knowledge about the big idea of an integral seems like it would help with understanding."
Definite integrals,Probabilistic Reasoning over Time (§14),The derivation of the closed forms for the forward step require on integration.
Definite integrals,Learning Probabilistic Models (§21),"The integral of a Gaussian is taken, and the fact that this must be equal to 1 is used."
Definite integrals,Robotics (§26),Integrals are used extensively in the planning and control section.
The fundamental theorem of calculus,Making Simple Decisions (§15),"On page 527, ""The probability density function is the derivative of the cumulative distribution function, so the density for X , the maximum of k estimates, is..."".  I would argue that to understand the relationship between the PDF and the CDF in probability, you really need to understand the fundamental theorem of calculus (at a conceptual level)."
Indefinite integrals and the net change theorem,Deep Learning (§22),Cross-entropy is defined in terms of an integral. Integrals are also used in the variational inference section to deal with KL divergence.
Integrals of exponential and logarithmic functions,Probabilistic Reasoning over Time (§14),"The one-step predicted distribution, expressed as an integral, is computed. (page 499)"
Integrals of exponential and logarithmic functions,Learning Probabilistic Models (§21),"A probability, expressed as an integral, is computed. (page 785)"
Calculus II,,
Application to probability,Probabilistic Reasoning (§13),A soft thereshold is defined using the integral of the standard normal distribution. (page 441)
Application to probability,Probabilistic Reasoning over Time (§14),The one-step predicted distribution is expressed as an integral. (page 497)
Application to probability,Making Simple Decisions (§15),The concept of an action stochastically domining another action is formally defined using integrals. (page 531) An example of stochastic dominacy is shown. (page 532) A reasoning based on integral is shown. (page 546)
Application to probability,Learning Probabilistic Models (§21),"A probability, expressed as an integral, is computed. (page 785)"
Application to probability,Deep Learning (§22),The cross-entry loss is defined an an integral. (page 809) The probability is expressed as an integral. (page 828) The KL divergence is expressed as an integral. (page 829)
Application to probability,Robotics (§26),The recursive filtering equation is expressed using an integral. (page 939) The functional J is expressed as an integral. (page 956) Several concepts are expressed as integrals
Introducing the concept of differential equations,Robotics (§26),A differential equation is solved to compute the theta function. (page 957)
Series,Making Complex Decisions (§16),A geometric series is used. (page 572)
Taylor series,Robotics (§26),"Page 942.  On the right, this function is approximated by a linear function. This linear function is tangent to f at the point  t , the mean of our state estimate at time t. Such a linearization is called first degree Taylor expansion."
Computer Graphics,,
Calculus topic,CS topic,"Rationale (Note: Unless mentioned otherwise, all chapters and sections refer to Marschner, Shirley, Fundamentals of Computer Graphics, 4th edition.)"
Calculus I,,
Limits at infinity and infinite limits,Defining/Storing/Rendering (§22) [all],Behaviour of Ricci blend at the two ends expressed as limits to infinity and negative infinity. (page 602)
"Continuity, discontinuities, and the intermediate value theorem","Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]","In graphics, we often deal with functions of a continuous variable. (page 183) Discontinuity in filters are discussed. (page 205)"
"Continuity, discontinuities, and the intermediate value theorem","Blending Functions, Bézier/Hermite, Catmull-Rom, Interpolating vs. Approximating, Subdivision (curves and surfaces), Picewise Curves/Splines (§15) [all]",Continuity. (page 366)
Motivating the need for the derivative and introducing the derivative concept,"Composition (§20) [§20.1, §20.2]",The approximation tan θ ≈ θ is used for small θ. (page 538)
Motivating the need for the derivative and introducing the derivative concept,"Vectors, Curves/Surfaces (§2) [all]",Understand that the gradient consists of several (partial) derivatives. (page 31) Derivative defined as a limit. (page 32) derivative interpreted as the slope of the tangent line. (page 32)
Motivating the need for the derivative and introducing the derivative concept,"Textures (§11) [§11.1, §11.2, §11.3, §11.4]",Jacobian consists of several (partial) derivatives. (page 262)
Motivating the need for the derivative and introducing the derivative concept,"Artistic Principles, Keyframing/Interpolation, Physically-Based, Spring-Mass Models, Behavioral models (§16) [all]",Jacobian consists of several (partial) derivatives. (page 421) Derivative and partial derivatives defined as a limit. (page 426)
Motivating the need for the derivative and introducing the derivative concept,"Blending Functions, Bézier/Hermite, Catmull-Rom, Interpolating vs. Approximating, Subdivision (curves and surfaces), Picewise Curves/Splines (§15) [all]",Points where the derivative is not continuous interpreted graphically. (page 367)
Motivating the need for the derivative and introducing the derivative concept,Defining/Storing/Rendering (§22) [all],Gradient consists of several (partial) derivatives. (page 589)
Defining the derivative as a function,"Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]","The first derivative of a function can be approximated by a finite difference, which then can be expressed as a convolution. "
Defining the derivative as a function,"Artistic Principles, Keyframing/Interpolation, Physically-Based, Spring-Mass Models, Behavioral models (§16) [all]","Graph of a function and its derivative. (page 411) Relationship between position function, velocity function, and acceleration function. (page 414)"
Basic differentiation rules,"Blending Functions, Bézier/Hermite, Catmull-Rom, Interpolating vs. Approximating, Subdivision (curves and surfaces), Picewise Curves/Splines (§15) [all]",Computation of first and second derivatives of a quadratic function. (page 372)
Derivatives of logarithmic and exponential functions,"Vectors, Curves/Surfaces (§2) [all]","The derivatives of logarithmic and exponential functions are presented in order to argue why the natural logarithm is ""natural."" (page 16)"
Implicit differentiation,"Vectors, Curves/Surfaces (§2) [all]",Implicit and explicit equations (page 30)
Implicit differentiation,"Blending Functions, Bézier/Hermite, Catmull-Rom, Interpolating vs. Approximating, Subdivision (curves and surfaces), Picewise Curves/Splines (§15) [all]",Implicit equation of a curve (page 360)
Implicit differentiation,Defining/Storing/Rendering (§22) [all],Implicit equation for a model (page 586)
The shape of graphs (concavity),"Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]","The second derivative of a function can be approximated by a finite difference, which then can be expressed as a convolution. "
Newton's method,Defining/Storing/Rendering (§22) [all],Newton's method applied to find intersections between rays and implicit functions.
Motivating the need for integrals and approximating the area under curves,"Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]",The integral that used in defining the convolution is interpreted as the area under the curve of the product of two functions. (page 194) The scaling of a filter uses some reasoning that relies on the connection between integrals and areas. (page 201)
Motivating the need for integrals and approximating the area under curves,Global Illumination (§23) [all],"The transport equation involves an integral. (pages 614, 617, 620)"
Definite integrals,"Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]",Moving averages smoothing is expresssed as an integral. (page 188) Convolution of two continuous functions is defined using an integral. (page 194) 2D convolutions are defined as integrals. (page 200)
Definite integrals,Defining/Storing/Rendering (§22) [all],Convolution surface expressed as an integral. (page 592)
The fundamental theorem of calculus,"Signal Processing Concepts (§9)  [§9.1, §9.2, §9.3, §9.4]",The computation of two box filter functions relies on computing an integral. (page 194)
"Using integrals for physical applications (work, force, density, mass, etc.)","Artistic Principles, Keyframing/Interpolation, Physically-Based, Spring-Mass Models, Behavioral models (§16) [all]",Hooke's law gives a model for springs.
Calculus II,,
Application to physics,Perception (§19) [all],"The tristimulus values (L, M, S) are expressed as integrals of a spectral composition function. (page 497) Saying that two spectral distributions generate the same color is expressed as 3 intergal equations. "
Introducing the concept of differential equations,Advanced Ray Tracing (§13) [all],Solve a differential equation. (page 325)
Introducing the concept of differential equations,"Artistic Principles, Keyframing/Interpolation, Physically-Based, Spring-Mass Models, Behavioral models (§16) [all]",Moving particle introduced as an ODE.
Parametric equations,"Vectors, Curves/Surfaces (§2) [all]",Parametric curves (pages 39-41)
Parametric equations,"Blending Functions, Bézier/Hermite, Catmull-Rom, Interpolating vs. Approximating, Subdivision (curves and surfaces), Picewise Curves/Splines (§15) [all]",Parametric equation of a curve (page 360) Arc-length parametrization. (page 363)
